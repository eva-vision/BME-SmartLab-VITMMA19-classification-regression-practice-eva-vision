# -*- coding: utf-8 -*-
"""Regression_GSE1U4_fin.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sLONqS_GYrSIxkXXF0Rkm9wmoEAjvoo5

# Copyright

<PRE>
This notebook was created as part of the "Deep learning / VITMMA19" class at
Budapest University of Technology and Economics, Hungary,
https://portal.vik.bme.hu/kepzes/targyak/VITMMA19.

Any re-use or publication of any part of the notebook is only allowed with the
written consent of the authors.

2023 (c) DÃ¡niel Unyi (unyi pont daniel kukac tmit pont bme pont hu)
</PRE>

# Regression example
This Jupyter Notebook contains a basic example of regression with linear regression, random forest and a deep neural network. The dataset we will use is the insurance dataset, which contains information about individuals and their medical insurance costs. We will preprocess the data, visualize it, and train different models on it. Finally, we will evaluate the models and compare their performance.
First, we install [Pytorch Lightning](https://lightning.ai/), which will be used as an easy-to-use interface to PyTorch.
"""

!pip install pytorch-lightning --quiet

"""## Data
Download the dataset:
"""

!wget https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv

"""## Imports
Importing libraries for data preprocessing & visualization:
"""

import pandas as pd # pandas for data manipulation
import numpy as np # numpy for linear algebra
import matplotlib.pyplot as plt # matplotlib for plotting
import seaborn as sns # seaborn for plotting

"""## DATA PREPROCESSING AND VISUALIZATION

A `df` dataframe is created by reading the contents of the "insurance.csv" file using pandas `read_csv()` method. The "insurance.csv" file contains information about individuals and their medical insurance costs. The `df` dataframe contains 1338 rows and 7 columns. Each row represents an individual and each column represents a feature of that individual such as age, sex, bmi, number of children, smoker or not, region and charges.
"""

df = pd.read_csv("insurance.csv")
df

df.info()

"""We have the column names already. There are no 'NaN' values in any of the columns.

The categorical columns contains strings, which have to convert to one-hot vectors. Fortunately, we can easily do this with pandas:
"""

df = pd.get_dummies(df, drop_first=True) # we have to talk about drop_first -> avoid the dummy variable trap
df

df.dtypes

df.iloc[:, :4] = df.iloc[:, :4].astype('float32')

df.dtypes

"""The following part generates a "heatmap" of pairwise correlations between the features in the `df` dataframe. The `sns.heatmap()` function from the seaborn library is used to create the heatmap. The `annot=True` parameter adds the correlation values to the heatmap.

Pairwise correlation is a measure of the linear relationship between two variables. In the context of linear regression, pairwise correlation is important because it helps us understand how each feature is related to the target variable. If two features are highly correlated, it means that they are providing similar information to the model, which can lead to overfitting. On the other hand, if two features are not correlated, it means that they are providing different information to the model, which can improve the model's performance. Therefore, it is important to check the pairwise correlations between the features before training a linear regression model.
"""

plt.figure(figsize=(12, 10))
plt.title('Pairwise correlations')
sns.heatmap(data=df.corr(), annot=True)
plt.show()

"""## Training linear regression and random forest regressors

We split the features and the target variable:

"""

X = df.drop(columns=['charges']).values # features
y = df['charges'].values # label

"""Now we split the dataset into train and test sets, and train linear regression and random forest regressors on the test set."""

from joblib.compressor import register_compressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler # alternative: min-max scaling
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
scaler.fit(X_train)

for regmodel in [LinearRegression(), RandomForestRegressor()]:
  regmodel.fit(scaler.transform(X_train), y_train)
  y_pred = regmodel.predict(scaler.transform(X_test))
  np.save(regmodel.__class__.__name__, y_pred)
  # regression plot
  plt.figure(figsize=(8, 8))
  sns.regplot(x=y_test, y=y_pred, scatter=True, color='b')
  plt.xlabel("True values")
  plt.ylabel("Predicted values")
  plt.xlim(min(y_test), max(y_test))
  plt.ylim(min(y_test), max(y_test))
  plt.title(f"Regression plot with {regmodel.__class__.__name__}")
  plt.show()
  # evaluation
  mae = mean_absolute_error(y_test, y_pred)
  print(f"Mean absolute error: {mae}")
  r2 = r2_score(y_test, y_pred)
  print(f"Coefficient of determination: {r2}")

"""## **ASSIGNMENT**: Regression with neural network

Your tasks are the following.


---


1. Train a FeedForward Neural Network with PyTorch Lightning. Use ModelCheckPoint as callback, and save the best performing model (in terms of MAE) into a ckpt file named *best_model.ckpt*. 10 points.


---


2. Save the MAE and the R2 score of your best performing model into a csv file called *mlp_results.csv*. The MAE must be lower and the R2 score must be higher than in linear regression. 10 points


---


3. Ensemble your neural network model with RandomForestRegressor following the formula
`y_pred_ensemble = 0.5 * y_pred_FeedForwardNet + 0.5 * y_pred_RandomForestRegressor`.
Save the MAE and the R2 score of the ensemble into a csv file called *ensemble_results.csv*. The MAE must be lower and the R2 score must be higher than for any of the two models. 10 points


---


All files must be saved directly into the git repository you submit.

Please check if these lines are working as expected before submission:

`model = FeedForwardNet.load_from_checkpoint('best_model.ckpt')`

`mlp_results = pd.read_csv('mlp_results.csv')`

`ensemble_results = pd.read_csv('ensemble_results.csv')`

Some thoughts on ensembles:
*   https://en.wikipedia.org/wiki/Ensemble_learning
*   https://ensemble-pytorch.readthedocs.io/
*   https://arxiv.org/abs/2106.03253
"""

#Train a FeedForward Neural Network with PyTorch Lightning.
#Use ModelCheckPoint as callback,
#and save the best performing model (in terms of MAE)
#into a ckpt file named best_model.ckpt. 10 points.

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
import pytorch_lightning as pl
import torchmetrics

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)

batch_size = 32
epochs = 200
lr = 0.002
hidden_dim = 32

def create_dataloader(X, y, batch_size):
  X = scaler.transform(X).astype('float32')
  X = torch.from_numpy(X)
  y = y.astype('float32')
  y = torch.from_numpy(y)
  dataset = TensorDataset(X, y)
  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
  return dataloader

train_loader = create_dataloader(X_train, y_train, batch_size)
val_loader = create_dataloader(X_val, y_val, batch_size)
test_loader = create_dataloader(X_test, y_test, batch_size)

# base class is the LightningModule, on which we build our custom model:
# - initialization: __init__ and setup()
# - train loop: training_step()
# - validation loop: validation_step()
# - test loop: test_step()
# - prediction loop: predict_step() - by default, runs the forward() method
# - optimizers and LR schedulers: configure_optimizers()

class MyFeedForwardNet(pl.LightningModule):
    def __init__(self, num_features, hidden_dim, lr):
      super(MyFeedForwardNet, self).__init__()
      self.save_hyperparameters()
      self.lr = lr
      self.layers = nn.Sequential(
          # simple logistic regression
          # incremental modeling:
          # we extend this with linear layers, nonlinearities, and possibly other layers
          # number of parameters vs. number of data points

          nn.Linear(8, hidden_dim),
          nn.ReLU(),
          nn.Linear(hidden_dim, hidden_dim),
          nn.ReLU(),
          nn.Linear(hidden_dim, hidden_dim),
          nn.ReLU(),
          nn.Linear(hidden_dim, 1)
      )

      self.loss = nn.L1Loss(size_average='false', reduce='mean', reduction='mean')
      self.acc = torchmetrics.R2Score()

    # forward propagation
    def forward(self, x):
      return self.layers(x)

    # one step of training
    def training_step(self, batch, batch_idx):
      loss, acc = self._shared_eval_step(batch, batch_idx)
      metrics = {"acc": acc, "loss": loss}
      self.log_dict(metrics, prog_bar=True)
      return metrics

    # one step of validation
    def validation_step(self, batch, batch_idx):
      loss, acc = self._shared_eval_step(batch, batch_idx)
      metrics = {"val_acc": acc, "val_loss": loss}
      self.log_dict(metrics, prog_bar=True)
      return metrics

    # one step of test
    def test_step(self, batch, batch_idx):
      loss, acc = self._shared_eval_step(batch, batch_idx)
      metrics = {"test_acc": acc, "test_loss": loss}
      self.log_dict(metrics)
      return metrics

    def _shared_eval_step(self, batch, batch_idx):
      x, y = batch
      y_hat = self(x).squeeze() # squeeze the output to match the target shape
      loss = self.loss(y_hat, y) # calculate the loss
      acc = self.acc(y_hat, y) # calculate the accuracy
      return loss, acc

    # we use the AdamW optimizer
    def configure_optimizers(self):
      return torch.optim.AdamW(self.parameters(), lr=self.lr)

logger = pl.loggers.TensorBoardLogger("logs/", name="heart_disease_logs")

model = MyFeedForwardNet(X_train.shape[1], hidden_dim, lr)

# we use the ModelCheckpoint callback to save the best model
callback = pl.callbacks.ModelCheckpoint(
    monitor='val_loss',
    dirpath = '',
    filename = 'best_model',
)


# we use the Trainer class to train our model
trainer = pl.Trainer(
    logger=logger,
    max_epochs=epochs,
    log_every_n_steps=1,
    callbacks=[callback]
)

trainer.fit(model, train_loader, val_loader) # train the model

"""## **MLP results to file**"""

#Show Test Metrics and write them to .CSV

test_metrics = pd.DataFrame(trainer.test(model, test_loader))
test_metrics.to_csv('mlp_results.csv', sep=',')

"""## **Ensemble**

"""

# Convert X_test to the appropriate data type and shape for predictions
X_test_transformed = scaler.transform(X_test).astype('float32')

# Make predictions using MyFeedForwardNet
with torch.no_grad():
    X_test_tensor = torch.from_numpy(X_test_transformed)
    y_pred_MyFeedForwardNet_tensor = model(X_test_tensor)
y_pred_MyFeedForwardNet = y_pred_MyFeedForwardNet_tensor.squeeze().numpy()

# Make predictions using RandomForestRegressor
y_pred_RandomForestRegressor = regmodel.predict(X_test_transformed)

# Ensemble the predictions
y_pred_ensemble = 0.5 * y_pred_MyFeedForwardNet + 0.5 * y_pred_RandomForestRegressor

# Calculate metrics for the ensemble model
ensemble_mae = mean_absolute_error(y_test, y_pred_ensemble)
ensemble_r2 = r2_score(y_test, y_pred_ensemble)

# Create a DataFrame to store the results
ensemble_results = pd.DataFrame({'MAE': [ensemble_mae], 'R2 Score': [ensemble_r2]})

# Save the results to a CSV file
ensemble_results.to_csv('ensemble_results.csv', index=False)
print(ensemble_results)